{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here will be defined functions that will be used for each model, to avoid code redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE RESULTING EMBEDDINGS:\n",
    "# Visually: reduce dimension with PCA and then plot specific set of words\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Phrases\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a given number of words for the vocabulary for a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words(X, nr_words, model):\n",
    "    \n",
    "    pca = PCA(n_components=2) # to have x and y for plotting\n",
    "    \n",
    "    scatter_plot_points = pca.fit_transform(X)\n",
    "    some_words = scatter_plot_points[:nr_words]\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.scatter(some_words[:,0],some_words[:,1],linewidths=10,color='blue')\n",
    "    plt.xlabel(\"x\",size=15)\n",
    "    plt.ylabel(\"y\",size=15)\n",
    "    plt.title(\"Word Embedding Space\",size=20)\n",
    "    vocab=list(model.wv.vocab)[:nr_words]\n",
    "    for i, word in enumerate(vocab):\n",
    "        plt.annotate(word,xy=(some_words[i,0],some_words[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print a given number of closest words for a given list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(model, words, nr):\n",
    "    \n",
    "    print('Closest words for word list:', words)\n",
    "    print('---------------------------------')\n",
    "    for x in model.wv.most_similar(positive = words, topn = nr):\n",
    "        print(x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print a given number of closest words for a given list of words with cosmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_word_cosmul(model, words, nr):\n",
    "    \n",
    "    print('Closest cosmul words for word list:', words)\n",
    "    print('---------------------------------')\n",
    "    for x in model.wv.most_similar_cosmul(positive = words, topn = nr):\n",
    "        print(x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print most similar words for a given word for a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar_words(model, list_of_words):\n",
    "    for word in list_of_words:\n",
    "        print('Most similar words for word:', word)\n",
    "        print('---------------------------------')\n",
    "        for w in model.wv.most_similar(word):\n",
    "            print(w)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print closest words for these set of words - we will use same set of words for each model so we can compare them more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words_for_word_list(model):\n",
    "    \n",
    "    print_closest_words(model, ['boy', 'family'], 1)\n",
    "\n",
    "    print_closest_words(model, ['great', 'amazing', 'cool'], 3)\n",
    "\n",
    "    print_closest_words(model, ['movie', 'actor'], 5)\n",
    "\n",
    "    print_closest_words(model, ['bad', 'awful'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words_for_word_list_cosmul(model):\n",
    "    \n",
    "    print_closest_word_cosmul(model, ['boy', 'family'], 1)\n",
    "\n",
    "    print_closest_word_cosmul(model, ['great', 'amazing', 'cool'], 3)\n",
    "\n",
    "    print_closest_word_cosmul(model, ['movie', 'actor'], 5)\n",
    "\n",
    "    print_closest_word_cosmul(model, ['bad', 'awful'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find 7 closest words for a given glove word embedding - we will use this to compare with our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding, glove_vectors, number_of_words):\n",
    "    return sorted(glove_vectors.keys(), key=lambda word: spatial.distance.euclidean(glove_vectors[word], embedding))[1:number_of_words+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print a vector representation of a specific word from glove and our own three models - unigram, bigram and trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embeddings_glove(glove_vectors, model_1, model_1_2, model_1_3, word):\n",
    "    print(\"GloVe vectors for word: \", word)\n",
    "    print(glove_vectors.get(word).astype(float))\n",
    "    print()\n",
    "\n",
    "    print(\"Word2vec vectors with unigram model for word: \", word)\n",
    "    print(model_1[word])\n",
    "    print()\n",
    "\n",
    "    print(\"Word2vec vectors with bigram model for word: \", word)\n",
    "    print(model_1_2[word])\n",
    "    print()\n",
    "\n",
    "    print(\"Word2vec vectors with trigram model for word: \", word)\n",
    "    print(model_1_3[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print closest glove embeddings for a specific list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_glove_embeddings(glove_vectors, list_of_words, number_of_words):\n",
    "    for word in list_of_words:\n",
    "        print(\"Closest GloVe embeddings for word: \", word)\n",
    "        print(\"----------------------------------------------------------\")\n",
    "        x = find_closest_embeddings(glove_vectors[word], glove_vectors, number_of_words)\n",
    "        print(x)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot closest words (in 2D) for each word from a given list of words, as input we will use a number of similar words we want to plot and a dimensionality method we want to use - TSNE or PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_word_embedding_2D(dim_reduction_method, model, user_input=None, words=None, label=None, color_map=None, perplexity = 0, learning_rate = 0, iteration = 0, topn=5, sample=10):\n",
    "\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.vocab.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.vocab ]\n",
    "    \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "    \n",
    "    if(dim_reduction_method == 'TSNE'):\n",
    "        two_dim = TSNE(n_components = 2, random_state=0, perplexity = perplexity, learning_rate = learning_rate, n_iter = iteration).fit_transform(word_vectors)[:,:2]\n",
    "    else:\n",
    "        two_dim = PCA(random_state=0).fit_transform(word_vectors)[:,:2]\n",
    "        \n",
    "    data = []\n",
    "\n",
    "    count = 0\n",
    "    for i in range (len(user_input)):\n",
    "\n",
    "                trace = go.Scatter(\n",
    "                    x = two_dim[count:count+topn,0],  \n",
    "                    y = two_dim[count:count+topn,1],\n",
    "                    text = words[count:count+topn],\n",
    "                    name = user_input[i],\n",
    "                    textposition = \"top center\",\n",
    "                    textfont_size = 20,\n",
    "                    mode = 'markers+text',\n",
    "                    marker = {\n",
    "                        'size': 10,\n",
    "                        'opacity': 0.8,\n",
    "                        'color': 2\n",
    "                    }\n",
    "       \n",
    "                )\n",
    "                        \n",
    "                data.append(trace)\n",
    "                count = count+topn\n",
    "\n",
    "    trace_input = go.Scatter(\n",
    "                    x = two_dim[count:,0], \n",
    "                    y = two_dim[count:,1], \n",
    "                    text = words[count:],\n",
    "                    name = 'input words',\n",
    "                    textposition = \"top center\",\n",
    "                    textfont_size = 20,\n",
    "                    mode = 'markers+text',\n",
    "                    marker = {\n",
    "                        'size': 10,\n",
    "                        'opacity': 1,\n",
    "                        'color': 'black'\n",
    "                    }\n",
    "                    )\n",
    "            \n",
    "    data.append(trace_input)\n",
    "    \n",
    "# Configure the layout\n",
    "\n",
    "    layout = go.Layout(\n",
    "        margin = {'l': 0, 'r': 0, 'b': 0, 't': 0},\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "        x=1,\n",
    "        y=0.5,\n",
    "        font=dict(\n",
    "            family=\"Courier New\",\n",
    "            size=25,\n",
    "            color=\"black\"\n",
    "        )),\n",
    "        font = dict(\n",
    "            family = \" Courier New \",\n",
    "            size = 15),\n",
    "        autosize = False,\n",
    "        width = 1000,\n",
    "        height = 1000\n",
    "        )\n",
    "\n",
    "\n",
    "    plot_figure = go.Figure(data = data, layout = layout)\n",
    "    plot_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot closest words (in 3D) for each word from a given list of words, as input we will use a number of similar words we want to plot and a dimensionality method we want to use - TSNE or PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_word_embedding_3D(dim_reduction_method, model, user_input=None, words=None, label=None, color_map=None, perplexity = 0, learning_rate = 0, iteration = 0, topn=5, sample=10):\n",
    "\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.vocab.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.vocab ]\n",
    "    \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "    \n",
    "    if(dim_reduction_method == 'TSNE'):\n",
    "        three_dim = TSNE(n_components = 3, random_state=0, perplexity = perplexity, learning_rate = learning_rate, n_iter = iteration).fit_transform(word_vectors)[:,:3]\n",
    "    else:\n",
    "        three_dim = PCA(random_state=0).fit_transform(word_vectors)[:,:3]\n",
    "    \n",
    "    \n",
    "    data = []\n",
    "\n",
    "    count = 0\n",
    "    for i in range (len(user_input)):\n",
    "\n",
    "                trace = go.Scatter3d(\n",
    "                    x = three_dim[count:count+topn,0], \n",
    "                    y = three_dim[count:count+topn,1],  \n",
    "                    z = three_dim[count:count+topn,2],\n",
    "                    text = words[count:count+topn],\n",
    "                    name = user_input[i],\n",
    "                    textposition = \"top center\",\n",
    "                    textfont_size = 20,\n",
    "                    mode = 'markers+text',\n",
    "                    marker = {\n",
    "                        'size': 10,\n",
    "                        'opacity': 0.8,\n",
    "                        'color': 2\n",
    "                    }\n",
    "       \n",
    "                )\n",
    "                           \n",
    "                data.append(trace)\n",
    "                count = count+topn\n",
    "\n",
    "    trace_input = go.Scatter3d(\n",
    "                    x = three_dim[count:,0], \n",
    "                    y = three_dim[count:,1],  \n",
    "                    z = three_dim[count:,2],\n",
    "                    text = words[count:],\n",
    "                    name = 'input words',\n",
    "                    textposition = \"top center\",\n",
    "                    textfont_size = 20,\n",
    "                    mode = 'markers+text',\n",
    "                    marker = {\n",
    "                        'size': 10,\n",
    "                        'opacity': 1,\n",
    "                        'color': 'black'\n",
    "                    }\n",
    "                    )\n",
    "            \n",
    "    data.append(trace_input)\n",
    "    \n",
    "# Configure the layout\n",
    "\n",
    "    layout = go.Layout(\n",
    "        margin = {'l': 0, 'r': 0, 'b': 0, 't': 0},\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "        x=1,\n",
    "        y=0.5,\n",
    "        font=dict(\n",
    "            family=\"Courier New\",\n",
    "            size=25,\n",
    "            color=\"black\"\n",
    "        )),\n",
    "        font = dict(\n",
    "            family = \" Courier New \",\n",
    "            size = 15),\n",
    "        autosize = False,\n",
    "        width = 1000,\n",
    "        height = 1000\n",
    "        )\n",
    "\n",
    "\n",
    "    plot_figure = go.Figure(data = data, layout = layout)\n",
    "    plot_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append a word with its similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_list(sim_words, words):\n",
    "    \n",
    "    list_of_words = []\n",
    "    \n",
    "    for i in range(len(sim_words)):\n",
    "        \n",
    "        sim_words_list = list(sim_words[i])\n",
    "        sim_words_list.append(words)\n",
    "        sim_words_tuple = tuple(sim_words_list)\n",
    "        list_of_words.append(sim_words_tuple)\n",
    "        \n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of similar words for each word from input_words and call a function to plot them in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similar_words_in_2D(input_words, model, number_of_words, dim_reduction_method):\n",
    "\n",
    "    user_input = [x.strip() for x in input_words.split(',')]\n",
    "    result_word = []\n",
    "    \n",
    "    for words in user_input:\n",
    "        \n",
    "        # if we are dealing with glove model, which is type dictionary:\n",
    "        if type(model) == dict:\n",
    "            sim_words = return_embedded_glove_tuple(model, words, number_of_words)\n",
    "        \n",
    "        else:\n",
    "            sim_words = model.wv.most_similar(words, topn = number_of_words)\n",
    "            \n",
    "        sim_words = append_list(sim_words, words)\n",
    "        result_word.extend(sim_words)\n",
    "    \n",
    "    similar_word = [word[0] for word in result_word]\n",
    "    similarity = [word[1] for word in result_word] \n",
    "    similar_word.extend(user_input)\n",
    "    labels = [word[2] for word in result_word]\n",
    "    label_dict = dict([(y,x+1) for x,y in enumerate(set(labels))])\n",
    "    color_map = [label_dict[x] for x in labels]\n",
    "\n",
    "    display_word_embedding_2D(dim_reduction_method, model, user_input, similar_word, labels, color_map, 5, 500, 10000, number_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of similar words for each word from input_words and call a function to plot them in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similar_words_in_3D(input_words, model, number_of_words, dim_reduction_method):\n",
    "\n",
    "    user_input = [x.strip() for x in input_words.split(',')]\n",
    "    result_word = []\n",
    "    \n",
    "    for words in user_input:\n",
    "         # if we are dealing with glove model, which is type dictionary:\n",
    "        if type(model) == dict:\n",
    "            sim_words = return_embedded_glove_tuple(model, words, number_of_words)\n",
    "        \n",
    "        else:\n",
    "            sim_words = model.wv.most_similar(words, topn = number_of_words)\n",
    "\n",
    "        sim_words = append_list(sim_words, words)       \n",
    "        result_word.extend(sim_words)\n",
    "    \n",
    "    similar_word = [word[0] for word in result_word]\n",
    "    similarity = [word[1] for word in result_word] \n",
    "    similar_word.extend(user_input)\n",
    "    labels = [word[2] for word in result_word]\n",
    "    label_dict = dict([(y,x+1) for x,y in enumerate(set(labels))])\n",
    "    color_map = [label_dict[x] for x in labels]\n",
    "\n",
    "    display_word_embedding_3D(dim_reduction_method, model, user_input, similar_word, labels, color_map, 5, 500, 10000, number_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load gloVe vectors with 50d, 100d and 200d to compare to our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(file, glove_vectors):\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "\n",
    "        word  = values[0]\n",
    "        vectors = np.asarray(values[1:], \"float32\")\n",
    "        glove_vectors[word] = vectors\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a word and its distance from a given word and return it a a topule (word, distance_from_similar_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_embedded_glove_tuple(model, words, number_of_words):\n",
    "    closest_words_for_word = find_closest_embeddings(model[words],model,  number_of_words)\n",
    "    \n",
    "    tuple_list = []\n",
    "    # now find the distances between the given word and each similar word and make a touple:\n",
    "    for w in closest_words_for_word:\n",
    "        distance = spatial.distance.euclidean(model[w],model[words])\n",
    "        word_tuple = (w, distance)\n",
    "        tuple_list.append(word_tuple)\n",
    "    \n",
    "    return tuple_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot in 2D the same word for 2 different models (This is used in other scripts to compare the vector representation between our word2vec model and glove vector models) - Vector normalization is applied with minmax_scale, and the dimensionality reduction model which we want to apply is passed on as an argument. It can be 'TSNE' or 'PCA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_word_embedding_form_2_models_2D(dim_reduction_method, model1, model2, word, perplexity = 0, learning_rate = 0, iteration = 0, sample=10):\n",
    "\n",
    "    label = word\n",
    "    \n",
    "    word_vectors = np.array([model1[word], model2[word]])\n",
    "    \n",
    "    # Normalizing the vectors in range -1 to 1\n",
    "    emb_scaled = minmax_scale(word_vectors, feature_range=(-1, 1))\n",
    "    model_names = ['word2Vec', 'GloVe']\n",
    "    if(dim_reduction_method == 'TSNE'):\n",
    "        two_dim = TSNE(n_components = 2, random_state=0, perplexity = perplexity, learning_rate = learning_rate, n_iter = iteration).fit_transform(emb_scaled)[:,:2]\n",
    "    else:\n",
    "        two_dim = PCA(random_state=0).fit_transform(emb_scaled)[:,:2]\n",
    "        \n",
    "    data = []\n",
    "\n",
    "    count = 0\n",
    "    for i in range (len(model_names)):\n",
    "        trace = go.Scatter(\n",
    "            x = two_dim[i:i+1,0], \n",
    "            y = two_dim[i:i+1,1],\n",
    "            text = word,\n",
    "            name = model_names[i],\n",
    "            textposition = \"top center\",\n",
    "            textfont_size = 20,\n",
    "            mode = 'markers+text',\n",
    "            marker = {\n",
    "                'size': 10,\n",
    "                'opacity': 0.8,\n",
    "                'color': i+1\n",
    "            }\n",
    "       \n",
    "        )\n",
    "                        \n",
    "        data.append(trace)\n",
    "    \n",
    "# Configure the layout\n",
    "\n",
    "    layout = go.Layout(\n",
    "        margin = {'l': 0, 'r': 0, 'b': 0, 't': 0},\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "        x=1,\n",
    "        y=0.5,\n",
    "        font=dict(\n",
    "            family=\"Courier New\",\n",
    "            size=25,\n",
    "            color=\"black\"\n",
    "        )),\n",
    "        font = dict(\n",
    "            family = \" Courier New \",\n",
    "            size = 15),\n",
    "        autosize = False,\n",
    "        width = 1000,\n",
    "        height = 1000\n",
    "        )\n",
    "\n",
    "\n",
    "    plot_figure = go.Figure(data = data, layout = layout)\n",
    "    plot_figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_word_embedding_form_2_models_3D(dim_reduction_method, model1, model2, word, perplexity = 0, learning_rate = 0, iteration = 0, sample=10):\n",
    "\n",
    "    label = word\n",
    "    \n",
    "    word_vectors = np.array([model1[word], model2[word]])\n",
    "    \n",
    "    # Normalizing the vectors in range -1 to 1\n",
    "    emb_scaled = minmax_scale(word_vectors, feature_range=(-1, 1))\n",
    "    model_names = ['word2Vec', 'GloVe']\n",
    "    if(dim_reduction_method == 'TSNE'):\n",
    "        three_dim = TSNE(n_components = 3, random_state=0, perplexity = perplexity, learning_rate = learning_rate, n_iter = iteration).fit_transform(emb_scaled)[:,:3]\n",
    "    else:\n",
    "        three_dim = PCA(random_state=0).fit_transform(emb_scaled)[:,:3]\n",
    "        \n",
    "    data = []\n",
    "\n",
    "    count = 0\n",
    "    for i in range (len(model_names)):\n",
    "        trace = go.Scatter3d(\n",
    "            x = three_dim[i:i+1,0], \n",
    "            y = three_dim[i:i+1,1],\n",
    "            z = three_dim[i:i+1,2],\n",
    "            text = word,\n",
    "            name = model_names[i],\n",
    "            textposition = \"top center\",\n",
    "            textfont_size = 20,\n",
    "            mode = 'markers+text',\n",
    "            marker = {\n",
    "                'size': 10,\n",
    "                'opacity': 0.8,\n",
    "                'color': i+1\n",
    "            }\n",
    "       \n",
    "        )\n",
    "                        \n",
    "        data.append(trace)\n",
    "    \n",
    "# Configure the layout\n",
    "\n",
    "    layout = go.Layout(\n",
    "        margin = {'l': 0, 'r': 0, 'b': 0, 't': 0},\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "        x=1,\n",
    "        y=0.5,\n",
    "        font=dict(\n",
    "            family=\"Courier New\",\n",
    "            size=25,\n",
    "            color=\"black\"\n",
    "        )),\n",
    "        font = dict(\n",
    "            family = \" Courier New \",\n",
    "            size = 15),\n",
    "        autosize = False,\n",
    "        width = 1000,\n",
    "        height = 1000\n",
    "        )\n",
    "\n",
    "\n",
    "    plot_figure = go.Figure(data = data, layout = layout)\n",
    "    plot_figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
